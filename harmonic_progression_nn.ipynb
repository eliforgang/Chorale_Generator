{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe77ebb-0403-4b63-91b3-8eb5938f7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25f0663-0c0a-46d3-8c4d-7bd63c9fc134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I V6 I vi I6 IV IV6 I I IV viidim6 I6 viidim/V V vi6 ii V I IV iii6 IV65 viidim I VI64 V7 I V6 I I64 V I I I6 V V6 I I6 V I6 ii vi iii6 ii6 V I vi iii IV IV6 I viidim6 iii vi7 V65/V V I',\n",
       " 'I I I6 IV vi V6/V V V/V V/V V6 viidim/V V viidim6/V V6 vi6 V64 V/V V V I V6/vi vi V7 V/V I6 IV7 V vi V6 I64 viidim/V V vi V65/V V7 I',\n",
       " 'I vi V I VII/iii iii vi65 ii V7/V V I ii I6 iii V iii vi64 ii65 V V7 I V V viidim6/ii ii ii ii64 iihdim65/ii iihdim65/ii vi ii V/ii V/ii V6/ii V/ii ii ii ii6 viihdim65 viihdim7 V7/vi vi V/vi vi vi64 IV7 I6 viidim6 viidim I V V I6 IV IV6 iii6 IV65 V65 I ii65 V V7 I',\n",
       " 'I I V6 I IV6 I6 V7 I IV V IV6 I ii6 V V I I IV V6/V V V42 I6 I6 ii/IV IV I V/V I6 V vi6 V/V V I6 IV6 viihdim42 I64 V I6 V I',\n",
       " 'I IV viidim6 I V6 IV6 V65 I I I ii7 I6 I6 V iii vi7 viidim6/V V iii vi V I6 I V V6/vi vi V6 I V65/ii ii ii V/ii I V6 V V iii6 I6 V65/V V V42 I6 ii I viidim6 I6 V I',\n",
       " 'I vi iii IV V42 I6 ii65 V I I V6/vi V7/vi vi V42 I6 ii65 V I I IV viidim64 I viidim6/vi vi V6/vi vi viihdim65 V/vi vi V65/V V I64 V42 I6 vi65 V/V V iii vi I64 IV6 V6 ii64 iii6 I V7/IV IV I6 ii ii43 iidim6/ii VI/ii V/V V I vi ii IV/IV viihdim7/IV vi43 viidim/IV IV I V6/V viidim/V V I ii V7/V V iii vi viidim6/V V VII/iii iii ii6 IV7 viihdim7/V V V6/vi IV6 I65/V I V vi IV I64 V I',\n",
       " 'I V I I7 V/V I6 V65/V V vi6 V/V V I6 V I I42 IV6 I64 ii65 V I I IV6 viidim/IV IV I64 ii65 V I I V7/IV IV viihdim43 I6 IV6 I I V I I6 IV iii6 IV6 viidim V I V7/IV IV I I V65 I ii6 V I V I',\n",
       " 'I I viidim6 I6 I ii6 V I I vi ii6 iii7 vi7 viidim6 I6 V V6/vi I42 vi6 ii vi ii65 V I I viidim viihdim43/IV IV6 viifdim7/vi vi ii64 viifdim42/ii ii I6 IV V vi I V V I']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chorales = open('major_chorales.txt', 'r').read().splitlines() #chorales in the format I want\n",
    "chorales[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "290214d3-3a5d-48aa-a902-5ed8eb7f11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3630\n"
     ]
    }
   ],
   "source": [
    "chorale_lists = open('all_major_chorales.txt', 'r').read().splitlines() #chorales as lists with each new item a roman numeral\n",
    "print(len(chorale_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31183402-d2d6-4ef8-9c92-881843e65a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I V6 I vi I6 IV IV6 I I IV viidim6 I6 viidim/V V vi6 ii V I IV iii6 IV65 viidim I VI64 V7 I V6 I I64 V I I I6 V V6 I I6 V I6 ii vi iii6 ii6 V I vi iii IV IV6 I viidim6 iii vi7 V65/V V I',\n",
       " 'I I I6 IV vi V6/V V V/V V/V V6 viidim/V V viidim6/V V6 vi6 V64 V/V V V I V6/vi vi V7 V/V I6 IV7 V vi V6 I64 viidim/V V vi V65/V V7 I',\n",
       " 'I vi V I VII/iii iii vi65 ii V7/V V I ii I6 iii V iii vi64 ii65 V V7 I V V viidim6/ii ii ii ii64 iihdim65/ii iihdim65/ii vi ii V/ii V/ii V6/ii V/ii ii ii ii6 viihdim65 viihdim7 V7/vi vi V/vi vi vi64 IV7 I6 viidim6 viidim I V V I6 IV IV6 iii6 IV65 V65 I ii65 V V7 I',\n",
       " 'I I V6 I IV6 I6 V7 I IV V IV6 I ii6 V V I I IV V6/V V V42 I6 I6 ii/IV IV I V/V I6 V vi6 V/V V I6 IV6 viihdim42 I64 V I6 V I',\n",
       " 'I IV viidim6 I V6 IV6 V65 I I I ii7 I6 I6 V iii vi7 viidim6/V V iii vi V I6 I V V6/vi vi V6 I V65/ii ii ii V/ii I V6 V V iii6 I6 V65/V V V42 I6 ii I viidim6 I6 V I',\n",
       " 'I vi iii IV V42 I6 ii65 V I I V6/vi V7/vi vi V42 I6 ii65 V I I IV viidim64 I viidim6/vi vi V6/vi vi viihdim65 V/vi vi V65/V V I64 V42 I6 vi65 V/V V iii vi I64 IV6 V6 ii64 iii6 I V7/IV IV I6 ii ii43 iidim6/ii VI/ii V/V V I vi ii IV/IV viihdim7/IV vi43 viidim/IV IV I V6/V viidim/V V I ii V7/V V iii vi viidim6/V V VII/iii iii ii6 IV7 viihdim7/V V V6/vi IV6 I65/V I V vi IV I64 V I',\n",
       " 'I V I I7 V/V I6 V65/V V vi6 V/V V I6 V I I42 IV6 I64 ii65 V I I IV6 viidim/IV IV I64 ii65 V I I V7/IV IV viihdim43 I6 IV6 I I V I I6 IV iii6 IV6 viidim V I V7/IV IV I I V65 I ii6 V I V I',\n",
       " 'I I viidim6 I6 I ii6 V I I vi ii6 iii7 vi7 viidim6 I6 V V6/vi I42 vi6 ii vi ii65 V I I viidim viihdim43/IV IV6 viifdim7/vi vi ii64 viifdim42/ii ii I6 IV V vi I V V I']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chorale_sets = open('major_chorale_lists.txt', 'r').read().splitlines() #chorales as lists with each new item a roman numeral\n",
    "chorales[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e9aeece-1f09-4784-b072-9510f9fda720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of roman numerals\n",
    "nums = [] \n",
    "for i in range(len(chorale_lists)):\n",
    "    if chorale_lists[i] not in nums:\n",
    "        nums.append(chorale_lists[i])\n",
    "nums = sorted(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0ffb4f6-a8e0-4eb3-9614-b13f1f3b0cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<e>', 1: '<s>', 2: 'I', 3: 'I42', 4: 'I43', 5: 'I43/V', 6: 'I6', 7: 'I6/viidim', 8: 'I64', 9: 'I65', 10: 'I65/V', 11: 'I7', 12: 'III6/viidim', 13: 'IV', 14: 'IV/IV', 15: 'IV/iii', 16: 'IV/vi', 17: 'IV/viidim', 18: 'IV42', 19: 'IV42/iii', 20: 'IV42/vi', 21: 'IV43', 22: 'IV43/iii', 23: 'IV6', 24: 'IV6/IV', 25: 'IV6/vi', 26: 'IV64', 27: 'IV64/ii', 28: 'IV65', 29: 'IV65/iii', 30: 'IV65/vi', 31: 'IV7', 32: 'V', 33: 'V/V', 34: 'V/ii', 35: 'V/vi', 36: 'V42', 37: 'V42/IV', 38: 'V42/V', 39: 'V43', 40: 'V43/V', 41: 'V43/iii', 42: 'V6', 43: 'V6/V', 44: 'V6/ii', 45: 'V6/vi', 46: 'V64', 47: 'V64/V', 48: 'V65', 49: 'V65/IV', 50: 'V65/V', 51: 'V65/ii', 52: 'V65/vi', 53: 'V7', 54: 'V7/IV', 55: 'V7/V', 56: 'V7/ii', 57: 'V7/iii', 58: 'V7/vi', 59: 'VI/ii', 60: 'VI42/viidim', 61: 'VI6/ii', 62: 'VI7/ii', 63: 'VII/iii', 64: 'VII/viidim', 65: 'VII6/iii', 66: 'VII6/viidim', 67: 'VII65/iii', 68: 'VII7/ii', 69: 'VII7/iii', 70: 'Vflat6/iii', 71: 'ii', 72: 'ii/IV', 73: 'ii42', 74: 'ii43', 75: 'ii6', 76: 'ii6/IV', 77: 'ii64', 78: 'ii65', 79: 'ii65/IV', 80: 'ii7', 81: 'ii7/IV', 82: 'iidim/viidim', 83: 'iidim6/ii', 84: 'iidim6/iii', 85: 'iihdim42/iii', 86: 'iihdim43/ii', 87: 'iihdim65/ii', 88: 'iihdim7/iii', 89: 'iii', 90: 'iii/V', 91: 'iii42', 92: 'iii43', 93: 'iii43/V', 94: 'iii6', 95: 'iii6/V', 96: 'iii64', 97: 'iii65', 98: 'iii7', 99: 'iii7/V', 100: 'iv/ii', 101: 'iv42/ii', 102: 'iv7/ii', 103: 'v/iii', 104: 'v6/iii', 105: 'vi', 106: 'vi42', 107: 'vi43', 108: 'vi6', 109: 'vi64', 110: 'vi65', 111: 'vi7', 112: 'viidim', 113: 'viidim/IV', 114: 'viidim/V', 115: 'viidim/ii', 116: 'viidim/vi', 117: 'viidim6', 118: 'viidim6/IV', 119: 'viidim6/V', 120: 'viidim6/ii', 121: 'viidim6/iii', 122: 'viidim6/vi', 123: 'viidim64', 124: 'viidim64/IV', 125: 'viidim64/V', 126: 'viidim64/vi', 127: 'viifdim42/ii', 128: 'viifdim42/vi', 129: 'viifdim43/ii', 130: 'viifdim43/vi', 131: 'viifdim65/ii', 132: 'viifdim7/V', 133: 'viifdim7/ii', 134: 'viifdim7/iii', 135: 'viifdim7/vi', 136: 'viihdim42', 137: 'viihdim42/V', 138: 'viihdim43', 139: 'viihdim43/IV', 140: 'viihdim43/V', 141: 'viihdim43/ii', 142: 'viihdim65', 143: 'viihdim65/V', 144: 'viihdim65/ii', 145: 'viihdim7', 146: 'viihdim7/IV', 147: 'viihdim7/V', 148: 'viihdim7/ii'}\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "stoi = {s:i for i,s in enumerate(nums)}\n",
    "# stoi[','] = 150\n",
    "# stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a1311e7-6bd3-42c6-a0f3-5861eff50bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ef3b75bb-8feb-47e3-937d-7c047fdd251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "# data = []\n",
    "# train_data = []\n",
    "# val_data = []\n",
    "\n",
    "# for i in range(len(chorale_sets)):\n",
    "#     datai = torch.tensor(encode(chorale_sets[i]), dtype=torch.long)\n",
    "#     n = int(0.9*len(datai)) # first 90% will be train, rest val\n",
    "#     train_data.append(datai[:n])\n",
    "#     val_data.append(datai[n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "89f656d0-4956-4c6f-af79-6ef97ad36e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(chorale_lists), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4b3ee593-45ef-4c97-95fa-285236e33e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e5593322-3e7a-4dc8-8f2f-0487836ad900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 4]), torch.Size([32, 4]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = get_batch('train')\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "724be128-d5e1-48c9-bfb0-abe7befbfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "892105b5-4fb3-4b0e-87b3-5de7434ac236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "96169c9c-2e09-40bb-aab1-5d48ade77fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dd22ea7c-4ddc-4300-9fa3-63a329e65bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "07394cbb-2429-4f98-9c21-3d7d187c66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dc57cca9-b765-452e-aa93-edeb479d1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 4 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 5e-5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "26f4e2c1-54bb-4cff-a455-1caf2d76780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.218773 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "326d19ef-00e9-4a6a-96eb-f7e0400291e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.2580, val loss 5.2069\n",
      "step 100: train loss 4.1438, val loss 4.1372\n",
      "step 200: train loss 3.7692, val loss 3.7848\n",
      "step 300: train loss 3.5903, val loss 3.5779\n",
      "step 400: train loss 3.4444, val loss 3.4868\n",
      "step 500: train loss 3.3494, val loss 3.4011\n",
      "step 600: train loss 3.2509, val loss 3.3164\n",
      "step 700: train loss 3.1891, val loss 3.2119\n",
      "step 800: train loss 3.1276, val loss 3.1721\n",
      "step 900: train loss 3.0570, val loss 3.1256\n",
      "step 1000: train loss 3.0191, val loss 3.0701\n",
      "step 1100: train loss 2.9631, val loss 3.0517\n",
      "step 1200: train loss 2.9355, val loss 3.0351\n",
      "step 1300: train loss 2.8925, val loss 3.0212\n",
      "step 1400: train loss 2.8831, val loss 2.9828\n",
      "step 1500: train loss 2.8309, val loss 2.9929\n",
      "step 1600: train loss 2.7941, val loss 2.9590\n",
      "step 1700: train loss 2.8064, val loss 2.9576\n",
      "step 1800: train loss 2.7414, val loss 2.9361\n",
      "step 1900: train loss 2.7092, val loss 2.9332\n",
      "step 2000: train loss 2.6973, val loss 2.9136\n",
      "step 2100: train loss 2.6672, val loss 2.9276\n",
      "step 2200: train loss 2.6441, val loss 2.8923\n",
      "step 2300: train loss 2.6520, val loss 2.8870\n",
      "step 2400: train loss 2.6143, val loss 2.8995\n",
      "step 2500: train loss 2.5998, val loss 2.8926\n",
      "step 2600: train loss 2.5742, val loss 2.9175\n",
      "step 2700: train loss 2.5871, val loss 2.8942\n",
      "step 2800: train loss 2.5531, val loss 2.8531\n",
      "step 2900: train loss 2.5445, val loss 2.8812\n",
      "step 3000: train loss 2.5294, val loss 2.8592\n",
      "step 3100: train loss 2.5232, val loss 2.8734\n",
      "step 3200: train loss 2.5100, val loss 2.8547\n",
      "step 3300: train loss 2.4950, val loss 2.8777\n",
      "step 3400: train loss 2.4599, val loss 2.8592\n",
      "step 3500: train loss 2.4586, val loss 2.8708\n",
      "step 3600: train loss 2.4506, val loss 2.8500\n",
      "step 3700: train loss 2.4249, val loss 2.8496\n",
      "step 3800: train loss 2.4161, val loss 2.8525\n",
      "step 3900: train loss 2.4018, val loss 2.8470\n",
      "step 4000: train loss 2.3911, val loss 2.8527\n",
      "step 4100: train loss 2.3830, val loss 2.8305\n",
      "step 4200: train loss 2.3626, val loss 2.8150\n",
      "step 4300: train loss 2.3499, val loss 2.8458\n",
      "step 4400: train loss 2.3530, val loss 2.8590\n",
      "step 4500: train loss 2.3267, val loss 2.8502\n",
      "step 4600: train loss 2.3091, val loss 2.8682\n",
      "step 4700: train loss 2.2879, val loss 2.8469\n",
      "step 4800: train loss 2.2820, val loss 2.8631\n",
      "step 4900: train loss 2.2768, val loss 2.8675\n",
      "step 4999: train loss 2.2909, val loss 2.8726\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4d14f1ec-7196-4a1e-a689-78e0a721235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<e> <s> I V ii42 I I IV I IV ii V viidim6 vi V I V I I6 IV vi7 ii I6 V V I6 iii IV6 I V I iidim6/iii viidim6/iii viifdim42/ii V I vi6 viidim6/vi vi vi IV V/vi vi IV vi ii65 V I V/V V I I65 IV viidim64 V6 vi viihdim42 viihdim7 I V6 I IV I6 V7 I IV6 I64 ii65 I6 V65/V V I IV I viidim6 V65 vi65 V/V V I V6/ii ii V6 vi7 viihdim7 I IV V V65/V I6 V V7 I vi I IV V I IV6 V6 I ii6 viidim6/IV V64 vi65 V/V V V iii7 IV IV6 I V IV6 I V V V7 I <e> <s> I vi vi V65/V V V I6 V VII6/iii iii V6 vi65 V/V V V I viidim6/V V V I ii viidim6/V V6 vi65 viidim64 viihdim43/IV viifdim7/iii iii IV42/vi V64 vi65 V64 ii65 V I <e> <s> I I6 V65/V V V V V/V I64 iv7/ii iii6 V7 I IV V I6 ii ii42 I I V6 I IV6 I64 ii65 V I ii64 viihdim7 viidim6/iii v/iii viidim6/V V6 vi65 V/V V I I viidim6/V V V6 vi65 V/V V\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740339b0-9be9-4b3d-b725-76b8dc9c3eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
